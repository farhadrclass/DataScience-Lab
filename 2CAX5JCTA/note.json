{"paragraphs":[{"text":"%md\n\n## Spark Core (RDDs) and Spark SQL Module\n#### Analyzing a Text File\n\n**Level**: Beginner\n**Language**: Python\n**Requirements**: \n- [HDP 2.6](http://hortonworks.com/products/sandbox/) (or later) or [HDCloud](https://hortonworks.github.io/hdp-aws/)\n- Spark 2.x\n\n**Author**: Robert Hryniewicz\n**Follow** [@RobertH8z](https://twitter.com/RobertH8z)","user":"admin","dateUpdated":"2017-02-22T10:09:35+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"tableHide":false,"title":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Spark Core (RDDs) and Spark SQL Module</h2>\n<h4>Analyzing a Text File</h4>\n<p><strong>Level</strong>: Beginner<br/><strong>Language</strong>: Python<br/><strong>Requirements</strong>:<br/>- <a href=\"http://hortonworks.com/products/sandbox/\">HDP 2.6</a> (or later) or <a href=\"https://hortonworks.github.io/hdp-aws/\">HDCloud</a><br/>- Spark 2.x</p>\n<p><strong>Author</strong>: Robert Hryniewicz<br/><strong>Follow</strong> <a href=\"https://twitter.com/RobertH8z\">@RobertH8z</a></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163817_1077079436","id":"20160331-233830_1876799966","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:09:12+0000","dateFinished":"2017-02-22T10:09:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:33601"},{"title":"Introduction","text":"%md\nThis lab consists of two parts. In each section you will perform a basic Word Count.\n\nIn **Part 1**, we will introduce **Resilient Distributed Datasets** (RDDs), Spark's primary low-level abstraction, and several core concepts.\nIn **Part 2**, we will introduce **DataFrames**, a higher-level abstraction than RDDs, along with Spark SQL Module allowing you to use SQL statements to query a temporary view.","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":217,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>This lab consists of two parts. In each section you will perform a basic Word Count.</p>\n<p>In <strong>Part 1</strong>, we will introduce <strong>Resilient Distributed Datasets</strong> (RDDs), Spark&rsquo;s primary low-level abstraction, and several core concepts.<br/>In <strong>Part 2</strong>, we will introduce <strong>DataFrames</strong>, a higher-level abstraction than RDDs, along with Spark SQL Module allowing you to use SQL statements to query a temporary view.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163817_1077079436","id":"20160331-233830_1038788941","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:05+0000","dateFinished":"2017-02-22T10:05:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33602"},{"title":"Concepts","text":"%md\nAt the core of Spark is the notion of a Resilient Distributed Dataset (RDD), which is an immutable and fault-tolerant collection of objects that is partitioned and distributed across multiple physical nodes on a cluster and they run in parallel.\n\nTypically, RDDs are instantiated by loading data from a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat on a YARN cluster.\n\nOnce an RDD is instantiated, you can apply a **[series of operations](https://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)**.\n\nAll operations fall into one of two types: **[Transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations)** or **[Actions](https://spark.apache.org/docs/latest/programming-guide.html#actions)**. \n\nTransformation operations, as the name suggests, create new datasets from an existing RDD and build out the processing Directed Acyclic Graph (DAG) that can then be applied on the partitioned dataset across the YARN cluster. An Action operation, on the other hand, executes DAG and returns a value.\n\nIn this lab we will use the following **[Transformations](https://spark.apache.org/docs/latest/programming-guide.html#transformations)**:\n- map(func)\n- filter(func)\n- flatMap(func)\n- reduceByKey(func)\n\nand **[Actions](https://spark.apache.org/docs/latest/programming-guide.html#actions)**:\n\n- collect()\n- count()\n- take()\n- takeOrdered(n, [ordering])\n- countByKey()\n\nA typical Spark application has the following four phases:\n1. Instantiate Input RDDs\n2. Transform RDDs\n3. Persist Intermediate RDDs\n4. Take Action on RDDs","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>At the core of Spark is the notion of a Resilient Distributed Dataset (RDD), which is an immutable and fault-tolerant collection of objects that is partitioned and distributed across multiple physical nodes on a cluster and they run in parallel.</p>\n<p>Typically, RDDs are instantiated by loading data from a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat on a YARN cluster.</p>\n<p>Once an RDD is instantiated, you can apply a <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#rdd-operations\">series of operations</a></strong>.</p>\n<p>All operations fall into one of two types: <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#transformations\">Transformations</a></strong> or <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#actions\">Actions</a></strong>. </p>\n<p>Transformation operations, as the name suggests, create new datasets from an existing RDD and build out the processing Directed Acyclic Graph (DAG) that can then be applied on the partitioned dataset across the YARN cluster. An Action operation, on the other hand, executes DAG and returns a value.</p>\n<p>In this lab we will use the following <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#transformations\">Transformations</a></strong>:<br/>- map(func)<br/>- filter(func)<br/>- flatMap(func)<br/>- reduceByKey(func)</p>\n<p>and <strong><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#actions\">Actions</a></strong>:</p>\n<ul>\n  <li>collect()</li>\n  <li>count()</li>\n  <li>take()</li>\n  <li>takeOrdered(n, [ordering])</li>\n  <li>countByKey()</li>\n</ul>\n<p>A typical Spark application has the following four phases:<br/>1. Instantiate Input RDDs<br/>2. Transform RDDs<br/>3. Persist Intermediate RDDs<br/>4. Take Action on RDDs</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163818_1078233682","id":"20160331-233830_2031164924","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:05+0000","dateFinished":"2017-02-22T10:05:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33603"},{"title":"How to run a paragraph?","text":"%md\nTo run a paragraph in a Zeppelin notebook you can either click the `play` button (blue triangle) on the right-hand side or simply press `Shift + Enter`.","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>To run a paragraph in a Zeppelin notebook you can either click the <code>play</code> button (blue triangle) on the right-hand side or simply press <code>Shift + Enter</code>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163818_1078233682","id":"20160331-233830_981276249","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:05+0000","dateFinished":"2017-02-22T10:05:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33604"},{"title":"Check Spark Version (should be 2.x)","text":"%spark2\n\nspark.version","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"editorMode":"ace/mode/scala","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163818_1078233682","id":"20160331-233830_1782991630","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:05+0000","dateFinished":"2017-02-22T10:05:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33605","errorMessage":""},{"text":"%md \n\n## Part 1\n#### Introduction to RDDs (Spark Core) with Word Count example","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 1</h2>\n<h4>Introduction to RDDs (Spark Core) with Word Count example</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163818_1078233682","id":"20160331-233830_682697678","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:05+0000","dateFinished":"2017-02-22T10:05:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33606"},{"text":"%md\nIn this section you will perform a basic word count with RDDs.\n\nYou will download external text data file to your sandbox. Then you will perform lexical analysis, or tokenization, by breaking up text into words/tokens.\nThe list of tokens then becomes an input for further processing to this and following sections.\n\nBy the end of this section you should have learned how to perform low-level transformations and actions with Spark RDDs and lambda expressions.","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In this section you will perform a basic word count with RDDs.</p>\n<p>You will download external text data file to your sandbox. Then you will perform lexical analysis, or tokenization, by breaking up text into words/tokens.<br/>The list of tokens then becomes an input for further processing to this and following sections.</p>\n<p>By the end of this section you should have learned how to perform low-level transformations and actions with Spark RDDs and lambda expressions.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163818_1078233682","id":"20160331-233830_94748225","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:05+0000","dateFinished":"2017-02-22T10:05:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33607"},{"title":"Interpreters","text":"%md\n\nIn the following paragraphs we are going to execute Spark code, run shell commands to download and move files, run sql queries etc. Each paragraph will start with `%` followed by an interpreter name, e.g. `%spark2.pyspark` for a Spark 2.x Python interpreter. Different interpreter names indicate what will be executed: code, markdown, html etc. This allows you to perform data ingestion, munging, wrangling, visualization, analysis, processing and more, all in one place!\n\nThroughtout this notebook we will use the following interpreters:\n\n- `%spark2.pyspark` - Spark Python interpreter to run Spark 2.x code written in Python 2.x\n- `%spark2` - Spark interpreter to run Spark 2.x code written in Scala (we'll only use to check Spark version)\n- `%spark2.sql` - Spark SQL interprter (to execute SQL queries against temporary tables in Spark 2.x)\n- `%sh` - Shell interpreter to run shell commands\n- `%angular` - Angular interpreter to run Angular and HTML code\n- `%md` - Markdown for displaying formatted text, links, and images\n\nTo learn more about Zeppelin interpreters check out this [link](https://zeppelin.apache.org/docs/0.5.6-incubating/manual/interpreters.html).","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In the following paragraphs we are going to execute Spark code, run shell commands to download and move files, run sql queries etc. Each paragraph will start with <code>%</code> followed by an interpreter name, e.g. <code>%spark2.pyspark</code> for a Spark 2.x Python interpreter. Different interpreter names indicate what will be executed: code, markdown, html etc. This allows you to perform data ingestion, munging, wrangling, visualization, analysis, processing and more, all in one place!</p>\n<p>Throughtout this notebook we will use the following interpreters:</p>\n<ul>\n  <li><code>%spark2.pyspark</code> - Spark Python interpreter to run Spark 2.x code written in Python 2.x</li>\n  <li><code>%spark2</code> - Spark interpreter to run Spark 2.x code written in Scala (we&rsquo;ll only use to check Spark version)</li>\n  <li><code>%spark2.sql</code> - Spark SQL interprter (to execute SQL queries against temporary tables in Spark 2.x)</li>\n  <li><code>%sh</code> - Shell interpreter to run shell commands</li>\n  <li><code>%angular</code> - Angular interpreter to run Angular and HTML code</li>\n  <li><code>%md</code> - Markdown for displaying formatted text, links, and images</li>\n</ul>\n<p>To learn more about Zeppelin interpreters check out this <a href=\"https://zeppelin.apache.org/docs/0.5.6-incubating/manual/interpreters.html\">link</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163818_1078233682","id":"20160331-233830_1148035148","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:05+0000","dateFinished":"2017-02-22T10:05:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33608"},{"title":"Download dataset to local storage","text":"%sh\n\n#  Remove old dataset file if already exists in local /tmp directory\nif [ -e /tmp/nifi.txt ]\nthen\n    rm -f /tmp/nifi.txt\nfi\n\n# Download \"About-Apache-NiFi\" text file\nwget https://raw.githubusercontent.com/roberthryniewicz/datasets/master/About-Apache-NiFi.txt -O /tmp/nifi.txt","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"editorMode":"ace/mode/sh","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"sh"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163819_1077848933","id":"20160331-233830_2033647788","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:05+0000","dateFinished":"2017-02-22T10:05:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33609","errorMessage":""},{"title":"Preview Downloaded Text File","text":"%sh\n\ncat /tmp/nifi.txt | head","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"editorMode":"ace/mode/sh","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"sh"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163819_1077848933","id":"20160331-233830_168647264","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33610","errorMessage":""},{"title":"Move file to HDFS (if supported/available)","text":"%sh\n\n# Remove old dataset if already exists in hadoop /tmp directory\nif hdfs dfs -stat /tmp/nifi.txt\nthen\n   hdfs dfs -rm  /tmp/nifi.txt\nfi\n\n# Move dataset to hadoop /tmp\nhdfs dfs -put /tmp/nifi.txt /tmp","user":"admin","dateUpdated":"2017-02-22T10:05:42+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"sh"},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487754154493_253384189","id":"20170222-090234_1186163827","dateCreated":"2017-02-22T09:02:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33611","dateFinished":"2017-02-22T10:05:12+0000","dateStarted":"2017-02-22T10:05:06+0000","errorMessage":""},{"title":"SparkSession in Zeppelin","text":"%md\nNote that the main entry point (starting with Spark 2.x) is `spark` (for SparkSession) and it is automatically initialized within Zeppelin.","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Note that the main entry point (starting with Spark 2.x) is <code>spark</code> (for SparkSession) and it is automatically initialized within Zeppelin.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163819_1077848933","id":"20160331-233830_1923635655","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:05+0000","dateFinished":"2017-02-22T10:05:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33612"},{"title":"Read Text File and Preview its Contents","text":"%spark2.pyspark\n\n# Parallelize text file using pre-initialized SparkSession\nlines = spark.sparkContext.textFile(\"/tmp/nifi.txt\")\n\n# Take a look at a few lines with a take() action.\nprint lines.take(10)","user":"admin","dateUpdated":"2017-02-22T10:05:05+0000","config":{"tableHide":false,"editorMode":"ace/mode/python","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163819_1077848933","id":"20160331-233830_541232082","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33613","errorMessage":""},{"title":"Note: 'u' stands for Unicode Characters","text":"%md\n\nIn the output above, each printed line is prefixed with **`u`**, which simply stands for **Unicode Characters** that represent a standard way of encoding text. You can read more [here](https://en.wikipedia.org/wiki/Unicode) if you're interested, but for the purpose of this and all other tutorials in the series you can **simply ignore it**.","user":"admin","dateUpdated":"2017-02-22T10:06:06+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":true,"language":"markdown"},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In the output above, each printed line is prefixed with <strong><code>u</code></strong>, which simply stands for <strong>Unicode Characters</strong> that represent a standard way of encoding text. You can read more <a href=\"https://en.wikipedia.org/wiki/Unicode\">here</a> if you&rsquo;re interested, but for the purpose of this and all other tutorials in the series you can <strong>simply ignore it</strong>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487752991888_1864548655","id":"20170222-084311_930733158","dateCreated":"2017-02-22T08:43:11+0000","dateStarted":"2017-02-22T10:06:06+0000","dateFinished":"2017-02-22T10:06:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33614"},{"text":"%md\nIn the next paragraphs we will start using Python lambda (or anonymous) functions. If you're unfamiliar with lambda expressions, \nreview **[Python Lambda Expressions](https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions)** before proceeding.","user":"admin","dateUpdated":"2017-02-22T10:05:06+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In the next paragraphs we will start using Python lambda (or anonymous) functions. If you&rsquo;re unfamiliar with lambda expressions,<br/>review <strong><a href=\"https://docs.python.org/2/tutorial/controlflow.html#lambda-expressions\">Python Lambda Expressions</a></strong> before proceeding.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163819_1077848933","id":"20160331-233830_1894357129","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33615"},{"title":"Extract All Words from the Document","text":"%spark2.pyspark\n# Here we're tokenizing our text file by using the split() function. Each original line of text is split into words or tokens on a single space.\n#  Also, since each line of the original text occupies a seperate bucket in the array, we need to use\n#  a flatMap() transformation to flatten all buckets into a asingle/flat array of tokens.\n\nwords = lines.flatMap(lambda line: line.split(\" \"))","user":"admin","dateUpdated":"2017-02-22T10:05:06+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163819_1077848933","id":"20160331-233830_2015200328","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33616","errorMessage":""},{"text":"%md\nNote that after you click `play` in the paragraph above \"nothing\" appears to happen.\n\nThat's because `flatMap()` is a **transformation** and all transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently – for example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n\nBy default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.","user":"admin","dateUpdated":"2017-02-22T10:05:06+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Note that after you click <code>play</code> in the paragraph above &ldquo;nothing&rdquo; appears to happen.</p>\n<p>That&rsquo;s because <code>flatMap()</code> is a <strong>transformation</strong> and all transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently – for example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.</p>\n<p>By default, each transformed RDD may be recomputed each time you run an action on it. However, you may also persist an RDD in memory using the persist (or cache) method, in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. There is also support for persisting RDDs on disk, or replicated across multiple nodes.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163819_1077848933","id":"20160331-233830_1507315859","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33617"},{"title":"Take a look at first 100 words","text":"%spark2.pyspark\nprint words.take(100)   # we're using a take(n) action\n\n# Output: As you can see, each word occupies a distinc array bucket.","user":"admin","dateUpdated":"2017-02-22T10:05:06+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163819_1077848933","id":"20160331-233830_1740542201","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33618","errorMessage":""},{"title":"Remove Empty Words","text":"%spark2.pyspark\n\nwordsFiltered = words.filter(lambda w: len(w) > 0)","user":"admin","dateUpdated":"2017-02-22T10:05:06+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"editorHide":false,"title":true,"results":[],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1487397163820_1075925189","id":"20160331-233830_270532773","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33619"},{"title":"Get Total Number of Words","text":"%spark2.pyspark\n\nprint wordsFiltered.count()     # using a count() action","user":"admin","dateUpdated":"2017-02-22T10:05:06+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163820_1075925189","id":"20160331-233830_229739488","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33620","errorMessage":""},{"text":"%md\n#### Word Counts\n\nLet's see what are the most popular words by performing a word count using `map()` and `reduceByKey()` transformations to create tuples of type (word, count).","user":"admin","dateUpdated":"2017-02-22T10:05:06+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Word Counts</h4>\n<p>Let&rsquo;s see what are the most popular words by performing a word count using <code>map()</code> and <code>reduceByKey()</code> transformations to create tuples of type (word, count).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163820_1075925189","id":"20160331-233830_55977510","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33621"},{"title":"Word count with a RDD","text":"%spark2.pyspark\n\nwordCounts = wordsFiltered.map(lambda word: (word, 1)).reduceByKey(lambda a,b: a+b)","user":"admin","dateUpdated":"2017-02-22T10:05:06+0000","config":{"tableHide":false,"editorMode":"ace/mode/python","colWidth":12,"title":false,"results":[],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1487397163820_1075925189","id":"20160331-233830_216173184","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33622"},{"text":"%md\n#### View Word Count Tuples\nNow let's take a look at top 100 words in descending order with a `takeOrdered()` action.","user":"admin","dateUpdated":"2017-02-22T10:05:06+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>View Word Count Tuples</h4>\n<p>Now let&rsquo;s take a look at top 100 words in descending order with a <code>takeOrdered()</code> action.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163820_1075925189","id":"20160331-233830_1029129342","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:06+0000","dateFinished":"2017-02-22T10:05:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33623"},{"text":"%spark2.pyspark\nprint wordCounts.takeOrdered(100, lambda (w,c): -c)\n","user":"admin","dateUpdated":"2017-02-22T10:05:06+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163820_1075925189","id":"20160331-233830_743558056","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:07+0000","dateFinished":"2017-02-22T10:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33624","errorMessage":""},{"text":"%md\n#### Filter out infrequent words\nWe'll use `filter()` transformation to filter out words that occur less than five times.","user":"admin","dateUpdated":"2017-02-22T10:05:07+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h4>Filter out infrequent words</h4>\n<p>We&rsquo;ll use <code>filter()</code> transformation to filter out words that occur less than five times.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163820_1075925189","id":"20160331-233830_772905299","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:07+0000","dateFinished":"2017-02-22T10:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33625"},{"text":"%spark2.pyspark\n\nfilteredWordCounts = wordCounts.filter(lambda (w,c): c >= 5)","user":"admin","dateUpdated":"2017-02-22T10:09:35+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"editorHide":false,"results":[],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"title":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1487397163820_1075925189","id":"20160331-233830_90779590","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:07+0000","dateFinished":"2017-02-22T10:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33626"},{"title":"Take a Look at Results","text":"%spark2.pyspark\n\nprint filteredWordCounts.collect()   # we're using a collect() action to pull everything back to the Spark driver","user":"admin","dateUpdated":"2017-02-22T10:05:07+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163821_1075540440","id":"20160331-233830_1024657848","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:07+0000","dateFinished":"2017-02-22T10:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33627","errorMessage":""},{"title":"","text":"%md\nNow let's use `countByKey()` action for another way of returning a word count.","user":"admin","dateUpdated":"2017-02-22T10:05:07+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now let&rsquo;s use <code>countByKey()</code> action for another way of returning a word count.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163821_1075540440","id":"20160331-233830_753086043","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:07+0000","dateFinished":"2017-02-22T10:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33628"},{"text":"%spark2.pyspark\n\nresult =  words.map(lambda w: (w,1)).countByKey()\n\n# Print type of data structure\nprint type(result)","user":"admin","dateUpdated":"2017-02-22T10:07:04+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163821_1075540440","id":"20160331-233830_1995992930","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:07+0000","dateFinished":"2017-02-22T10:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33629","errorMessage":"","title":"Print Data Structure Type"},{"text":"%md\n**Note** that the **result** is an **unordered dictionary of type {word, count}**.\nSince this is a small set we can apply a simple (non-parallelizeable) python built-in function.\n","user":"admin","dateUpdated":"2017-02-22T10:05:07+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p><strong>Note</strong> that the <strong>result</strong> is an <strong>unordered dictionary of type {word, count}</strong>.<br/>Since this is a small set we can apply a simple (non-parallelizeable) python built-in function.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163821_1075540440","id":"20160331-233830_811124723","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:07+0000","dateFinished":"2017-02-22T10:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33630"},{"text":"%md\nTake a look at first 20 items in our dictionary.","user":"admin","dateUpdated":"2017-02-22T10:05:07+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Take a look at first 20 items in our dictionary.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163821_1075540440","id":"20160331-233830_347028305","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:07+0000","dateFinished":"2017-02-22T10:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33631"},{"text":"%spark2.pyspark\n# Print first 20 items\nprint result.items()[0:20]","user":"admin","dateUpdated":"2017-02-22T10:05:07+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163821_1075540440","id":"20160331-233830_2086620530","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:07+0000","dateFinished":"2017-02-22T10:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33632","errorMessage":""},{"text":"%md\nApply a python `sorted()` function on the **result** dictionary values.","user":"admin","dateUpdated":"2017-02-22T10:05:07+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Apply a python <code>sorted()</code> function on the <strong>result</strong> dictionary values.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163821_1075540440","id":"20160331-233830_1423292200","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:07+0000","dateFinished":"2017-02-22T10:05:07+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33633"},{"text":"%spark2.pyspark\nimport operator\n\n# Sort in descending order\nsortedResult = sorted(result.items(), key=operator.itemgetter(1), reverse=True)\n\n# Print top 20 items\nprint sortedResult[0:20]","user":"admin","dateUpdated":"2017-02-22T10:05:07+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"title":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163822_1076694687","id":"20160331-233830_451661467","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33634","errorMessage":""},{"text":"%md\n## Part 2\n#### Introduction to DataFrames and SQL APIs (Spark SQL Module)","user":"admin","dateUpdated":"2017-02-22T10:05:08+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Part 2</h2>\n<h4>Introduction to DataFrames and SQL APIs (Spark SQL Module)</h4>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163822_1076694687","id":"20160331-233830_1867067371","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33635"},{"text":"%md\nIn this section we will cover the concept of a DataFrame. You will convert RDDs from a previous section and then use higher level \noperations to demonstrate a different way of counting words. Then you will register a temporary table and perform a word count by \nexecuting a SQL query on that table.\n\nBy the end of the section you will have learned higher-level Spark abstractions that hide lower-level details, speed up prototyping and execution. ","user":"admin","dateUpdated":"2017-02-22T10:05:08+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>In this section we will cover the concept of a DataFrame. You will convert RDDs from a previous section and then use higher level<br/>operations to demonstrate a different way of counting words. Then you will register a temporary table and perform a word count by<br/>executing a SQL query on that table.</p>\n<p>By the end of the section you will have learned higher-level Spark abstractions that hide lower-level details, speed up prototyping and execution.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163822_1076694687","id":"20160331-233830_770254433","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33636"},{"title":"DataFrame","text":"%md\nA DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. [See SparkSQL docs for more info](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes).","user":"admin","dateUpdated":"2017-02-22T10:05:08+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\">See SparkSQL docs for more info</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163822_1076694687","id":"20160331-233830_634831315","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33637"},{"text":"%md\nTransform your RDD into a DataFrame and perform DataFrame specific operations.","user":"admin","dateUpdated":"2017-02-22T10:05:08+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Transform your RDD into a DataFrame and perform DataFrame specific operations.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163822_1076694687","id":"20160331-233830_911152909","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33638"},{"title":"Word Count with a DataFrame","text":"%spark2.pyspark\n\n# First, let's transform our RDD to a DataFrame.\n# We will use a Row to define column names.\nwordsCounts = (filteredWordCounts.map(lambda (w, c): \n                Row(word=w,\n                    count=c))\n                .toDF())\n\n# Print schema\nwordsCounts.printSchema()\n\n# Output: As you can see, the count and word types have been inferred without having to explicitly define long and string types respectively.","user":"admin","dateUpdated":"2017-02-22T10:07:51+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163822_1076694687","id":"20160331-233830_41054806","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33639","errorMessage":""},{"title":"Show top 20 rows","text":"%spark2.pyspark\n\n# Show top 20 rows\nwordsCounts.show()","user":"admin","dateUpdated":"2017-02-22T10:05:08+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"editorHide":false,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163822_1076694687","id":"20160331-233830_665873755","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33640","errorMessage":""},{"title":"Register a Temporary View","text":"%spark2.pyspark\n\nwordsCounts.createOrReplaceTempView(\"wordcounts\")","user":"admin","dateUpdated":"2017-02-22T10:05:08+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1487397163822_1076694687","id":"20160331-233830_802915768","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33641"},{"text":"%md\nNow we can query the temporary `wordcounts` table with a SQL statement.","user":"admin","dateUpdated":"2017-02-22T10:05:08+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now we can query the temporary <code>wordcounts</code> table with a SQL statement.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163823_1076309938","id":"20160331-233830_1965558675","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33642"},{"text":"%md\nTo execute a SparkSQL query we prepend a block of SQL code with a `%sql` line.","user":"admin","dateUpdated":"2017-02-22T10:05:08+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>To execute a SparkSQL query we prepend a block of SQL code with a <code>%sql</code> line.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163823_1076309938","id":"20160331-233830_403708924","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33643"},{"title":"","text":"%spark2.sql\n\n-- Display word counts in descending order\nSELECT word, count FROM wordcounts ORDER BY count DESC","user":"admin","dateUpdated":"2017-02-22T10:05:08+0000","config":{"tableHide":false,"editorMode":"ace/mode/sql","colWidth":12,"title":false,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163823_1076309938","id":"20160331-233830_1235044795","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:08+0000","dateFinished":"2017-02-22T10:05:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33644","errorMessage":""},{"text":"%md\nNow let's take a step back and perform a word count with SQL","user":"admin","dateUpdated":"2017-02-22T10:05:08+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now let&rsquo;s take a step back and perform a word count with SQL</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163823_1076309938","id":"20160331-233830_1968421310","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:09+0000","dateFinished":"2017-02-22T10:05:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33645"},{"title":"Convert RDD to a DataFrame and Register a New Temp Table","text":"%spark2.pyspark\n\n# Convert wordsFiltered RDD to a Data Frame\nwords = wordsFiltered.map(lambda w: Row(word=w, count=1)).toDF()","user":"admin","dateUpdated":"2017-02-22T10:05:09+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"editorHide":false,"title":true,"results":[],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163823_1076309938","id":"20160331-233830_1271375135","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:09+0000","dateFinished":"2017-02-22T10:05:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33646","errorMessage":""},{"title":"Use DataFrame Specific Functions to Determine Word Counts","text":"%spark2.pyspark\n\n(words.groupBy(\"word\")\n        .sum()\n        .orderBy(\"sum(count)\", ascending=0)\n        .limit(10).show())","user":"admin","dateUpdated":"2017-02-22T10:05:09+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163823_1076309938","id":"20160331-233830_539606295","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:09+0000","dateFinished":"2017-02-22T10:05:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33647","errorMessage":""},{"title":"Register as Temporary View","text":"%spark2.pyspark\n\n# Create a Temporary View\nwords.createOrReplaceTempView(\"words\")","user":"admin","dateUpdated":"2017-02-22T10:05:09+0000","config":{"editorMode":"ace/mode/python","colWidth":12,"title":true,"results":[],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163823_1076309938","id":"20160331-233830_339558784","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:09+0000","dateFinished":"2017-02-22T10:05:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33648","errorMessage":""},{"title":"Word Count using SQL","text":"%md\n\nNow let's do a word count using a SQL statement against the `words` table and order the results in a descending order by count.","user":"admin","dateUpdated":"2017-02-22T10:05:09+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Now let&rsquo;s do a word count using a SQL statement against the <code>words</code> table and order the results in a descending order by count.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163824_1086698158","id":"20160331-233830_1100432609","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:09+0000","dateFinished":"2017-02-22T10:05:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33649"},{"text":"%spark2.sql\n\nSELECT word, count(*) as count FROM words GROUP BY word ORDER BY count DESC LIMIT 10","user":"admin","dateUpdated":"2017-02-22T10:08:27+0000","config":{"tableHide":false,"editorMode":"ace/mode/sql","colWidth":12,"editorHide":false,"results":[{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"keys":[{"name":"word","index":0,"aggr":"sum"}],"values":[{"name":"count","index":1,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"word","index":0,"aggr":"sum"},"yAxis":{"name":"count","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"sql"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163824_1086698158","id":"20160331-233830_841691499","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:08:27+0000","dateFinished":"2017-02-22T10:08:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33650","errorMessage":""},{"title":"The End","text":"%md\nYou've reached the end of this lab! We hope you've been able to successfully complete all the sections and learned a thing or two about Apache Spark: low-level RDD transformations and actions as well as higher-level DataFrame and SQL APIs.","user":"admin","dateUpdated":"2017-02-22T10:05:09+0000","config":{"editorMode":"ace/mode/markdown","colWidth":12,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>You&rsquo;ve reached the end of this lab! We hope you&rsquo;ve been able to successfully complete all the sections and learned a thing or two about Apache Spark: low-level RDD transformations and actions as well as higher-level DataFrame and SQL APIs.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163824_1086698158","id":"20160331-233830_293992216","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:09+0000","dateFinished":"2017-02-22T10:05:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33651"},{"title":"Additional Resources","text":"%md\n\nWe hope you've enjoyed this brief intro to Apache Spark. Below are additional resources that you should find useful:\n\n1. [Hortonworks Apache Spark Tutorials](http://hortonworks.com/tutorials/#tuts-developers) are your natural next step where you can explore Spark in more depth.\n2. [Hortonworks Community Connection (HCC)](https://community.hortonworks.com/spaces/85/data-science.html?type=question) is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.\n3. [Hortonworks Apache Spark Docs](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_spark-component-guide/content/ch_developing-spark-apps.html) - official Spark documentation.\n4. [Hortonworks Apache Zeppelin Docs](http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_zeppelin-component-guide/content/ch_using_zeppelin.html) - official Zeppelin documentation.\n","user":"admin","dateUpdated":"2017-02-22T10:05:09+0000","config":{"tableHide":false,"editorMode":"ace/mode/markdown","colWidth":10,"editorHide":true,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>We hope you&rsquo;ve enjoyed this brief intro to Apache Spark. Below are additional resources that you should find useful:</p>\n<ol>\n  <li><a href=\"http://hortonworks.com/tutorials/#tuts-developers\">Hortonworks Apache Spark Tutorials</a> are your natural next step where you can explore Spark in more depth.</li>\n  <li><a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\">Hortonworks Community Connection (HCC)</a> is a great resource for questions and answers on Spark, Data Analytics/Science, and many more Big Data topics.</li>\n  <li><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_spark-component-guide/content/ch_developing-spark-apps.html\">Hortonworks Apache Spark Docs</a> - official Spark documentation.</li>\n  <li><a href=\"http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.5.0/bk_zeppelin-component-guide/content/ch_using_zeppelin.html\">Hortonworks Apache Zeppelin Docs</a> - official Zeppelin documentation.</li>\n</ol>\n</div>"}]},"apps":[],"jobName":"paragraph_1487397163824_1086698158","id":"20160331-233830_1914786212","dateCreated":"2017-02-18T05:52:43+0000","dateStarted":"2017-02-22T10:05:09+0000","dateFinished":"2017-02-22T10:05:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33652"},{"text":"%angular\n</br>\n<center>\n<a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\" target='_blank'>\n  <img src=\"http://hortonworks.com/wp-content/uploads/2016/03/logo-hcc.png\" alt=\"HCC\" style=\"width:125px;height:125px;border:0;\" align=\"middle\">\n</a>\n</center>","dateUpdated":"2017-02-22T10:05:09+0000","config":{"editorMode":"ace/mode/scala","colWidth":2,"editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"ANGULAR","data":"</br>\n<center>\n<a href=\"https://community.hortonworks.com/spaces/85/data-science.html?type=question\" target='_blank'>\n  <img src=\"http://hortonworks.com/wp-content/uploads/2016/03/logo-hcc.png\" alt=\"HCC\" style=\"width:125px;height:125px;border:0;\" align=\"middle\">\n</a>\n</center>"}]},"apps":[],"jobName":"paragraph_1487397163824_1086698158","id":"20160331-233830_200815067","dateCreated":"2017-02-18T05:52:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33653","user":"admin","dateFinished":"2017-02-22T10:05:09+0000","dateStarted":"2017-02-22T10:05:09+0000"},{"text":"","dateUpdated":"2017-02-18T05:52:43+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1487397163824_1086698158","id":"20161018-144007_1720066531","dateCreated":"2017-02-18T05:52:43+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:33654"}],"name":"Labs / Spark 2.x / Data Worker / Python / 101 - Intro to Spark","id":"2CAX5JCTA","angularObjects":{"2C8KBC98A:shared_process":[],"2C8GUTGCE:shared_process":[],"2CBQPWEAP:shared_process":[],"2CATVSY4P:shared_process":[],"2CC55453H:shared_process":[],"2CA97ZN5K:shared_process":[],"2C4U48MY3_spark2:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}